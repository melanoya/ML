{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Домашнее задание №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузила данные, посмотрела на классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = 'SMSSpamCollection'\n",
    "messages = pandas.read_csv(path, sep='\\t', names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        message\n",
      "label                                                          \n",
      "ham   count                                                4825\n",
      "      unique                                               4516\n",
      "      top                                Sorry, I'll call later\n",
      "      freq                                                   30\n",
      "spam  count                                                 747\n",
      "      unique                                                653\n",
      "      top     Please call our customer service representativ...\n",
      "      freq                                                    4\n"
     ]
    }
   ],
   "source": [
    "# print(messages)\n",
    "print(messages.groupby('label').describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# messages['length'] = messages['message'].map(lambda text: len(text))\n",
    "# print(messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализовала данные: со знаками препинания. но! убрала штуки типа 'слово'+'..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 'слово'+'..'  плохо токенизируется функцией word_tokenize, поэтому уберем это\n",
    "messages['message'] = [re.sub('(\\w)\\.\\. ', '\\\\1 ', msg) for msg in messages['message']]\n",
    "# print(messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализовала данные: без знаков препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages['message'] = [re.sub('[!\"?/\\\\().:;,-]', '', msg) for msg in messages['message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализовала данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "messages['message'] = [re.sub('(\\w)\\.\\. ', '\\\\1 ', msg) for msg in messages['message']]\n",
    "messages['message'] = [word_tokenize(msg) for msg in messages['message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### стемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = []\n",
    "for msg in messages['message']:\n",
    "    msg = [lancaster_stemmer.stem(i.lower()) for i in msg]\n",
    "    arr.append(' '.join(msg))\n",
    "messages['message'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  go until jurong point , crazy avail on in bug ...\n",
      "1   ham                        ok lar ... jok wif u on ...\n",
      "2  spam  fre entry in 2 a wkly comp to win fa cup fin t...\n",
      "3   ham  u dun say so ear hor ... u c already then say ...\n",
      "4   ham  nah i do n't think he goe to usf , he liv arou...\n"
     ]
    }
   ],
   "source": [
    "print(messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = []\n",
    "for msg in messages['message']:\n",
    "    msg = [wordnet_lemmatizer.lemmatize(i.lower()) for i in msg]\n",
    "    arr.append(' '.join(msg))\n",
    "messages['message'] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  go until jurong point , crazy available only i...\n",
      "1   ham                    ok lar ... joking wif u oni ...\n",
      "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
      "3   ham  u dun say so early hor ... u c already then sa...\n",
      "4   ham  nah i do n't think he go to usf , he life arou...\n"
     ]
    }
   ],
   "source": [
    "print(messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализовала данные:  удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages['message'] = [re.sub('(\\w)\\.\\. ', '\\\\1 ', msg) for msg in messages['message']]\n",
    "messages['message'] = [[w for w in word_tokenize(msg) if not w in stopset] for msg in messages['message']]\n",
    "\n",
    "arr = []\n",
    "for msg in messages['message']:\n",
    "    msg = [w for w in msg if not w in stopset]\n",
    "    arr.append(' '.join(msg))\n",
    "messages['message'] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбираю функцию векторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit_transform(messages['message'])\n",
    "# print(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8690 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 52738 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = TfidfVectorizer()\n",
    "bow.fit_transform(messages['message'])\n",
    "# print(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построила модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*пы.сы. я запускаю каждый кусок нормализации данных отдельно вместе с запуском модели*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bowed_messages = bow.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973978901086 0.0069513986969\n"
     ]
    }
   ],
   "source": [
    "cv_results = cross_val_score(naive_model, bowed_messages, messages['label'], cv=10, scoring='accuracy')\n",
    "print(cv_results.mean(), cv_results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Выборка не сбалансирована. ham приблизительно в 6 раз больше, чем spam. Так что попробуем справиться с этим при помощи кроссвалидации, вот.\n",
    "2. Какой какой.. плохой очевидно \n",
    "3. разная токенизация (для простоты я смотрю на среднее значение, т.е. чем оно больше, тем лучше работает модель):\n",
    "    * *все модели построены при помощи CountVectorizer*\n",
    "        * **со знаками препинания:** mean: 0.980260320614    std: 0.00488300101375\n",
    "        * **без знаков препинания:** mean: 0.977745560545    std: 0.00522292832323\n",
    "        * **стемматизация + знаки препинания:** mean: 0.978646452155    std: 0.00411601196134\n",
    "        * **лемматизация + знаки препинания:** mean: 0.97900390639    std: 0.00488611860617\n",
    "        * **удаление стоп-слов:** mean: 0.983130280394    std: 0.00255807309826\n",
    "    * *модель со стоп-словами показывает лучший результат. построим её же, только при помощи TfidfTransformer*\n",
    "        * **удаление стоп-слов:** mean: 0.973978901086 std: 0.0069513986969"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
